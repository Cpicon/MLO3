{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit and HuggingFace Stater Kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============= PART 1 =============\n",
    "The first part of this excercise is to figure out how to instantiate a Reddit API object using the [PRAW library](https://praw.readthedocs.io/en/stable/). This is a Python library that gives easy to understand interfaces to interact with the Reddit API.\n",
    "\n",
    "Your first task is to look through the [documentation here](https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html) and figure out how to instanstiate a Reddit instance. (hint: you only need to use `client_id`, `client_secret`, and `user_agent`)\n",
    "\n",
    "Make sure everyone in the group does this part! Follow the guide below on how to get your `client_id` and `client_secret`.\n",
    "\n",
    "### Steps for part 1:\n",
    "1. Pull the `FourthBrain/ML03` repo locally so you can start development.\n",
    "2. Open `reddit_and_huggingface.ipynb` and install the necessary packages for this lesson by running:\n",
    "\n",
    "    ```\n",
    "    cd code_student/Week_2\n",
    "    conda activate {your_virtual_environment_name}\n",
    "    pip install transformers praw torch torchvision torchaudio\n",
    "    ```\n",
    "\n",
    "3. Get your `client_id` and `client_secret` from Reddit by doing:\n",
    "\n",
    "* Make a Reddit account\n",
    "* Follow the steps in this screenshot which are the first steps from this [guide](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c).\n",
    "\n",
    "![instructions to set up reddit api](../../images/reddit_get_access.JPG)\n",
    "\n",
    "* Create a `secrets.py` file and include the following:\n",
    "\n",
    "    ```\n",
    "    REDDIT_API_CLIENT_ID = \"\"\n",
    "    REDDIT_API_CLIENT_SECRET = \"\"\n",
    "    REDDIT_API_USER_AGENT = {can_be_any_string...for ex: \"marksbot\"}\n",
    "    ```\n",
    "\n",
    "* Put `secrets.py` in `Week_2` so you can easily import it\n",
    "\n",
    "4. Read the [documentation here](https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html) and try to figure out how to instanstiate a Reddit instance object. \n",
    "5. Lastly, once you have your Reddit instance object, figure out how to make a `subreddit` object for your favorite subreddit. (hint: look for `subreddit` in the documentation)\n",
    "6. If time permits, continue to browse the documentation to prep for the next part of this excercise which will be to get submissions from the subreddit of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============= PART 2 =============\n",
    "\n",
    "This next part you are going to figure out how to parse comments by using your `subreddit` instance object.\n",
    "\n",
    "### Todos for part 2:\n",
    "1. How do I find the top 10 posts of all time from your favorite subreddit(s)? (hint: look at [\"Obtain Submission Instances from a Subreddit\"](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html))\n",
    "2. How do I parse comments from the post? (hint: look at [\"Obtain Submission Instances from a Subreddit\"](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html))\n",
    "3. And finally, how do I parse replies from that comment? (hint: look at [this tutorial](https://praw.readthedocs.io/en/stable/tutorials/comments.html))\n",
    "\n",
    "Put your scraped data into a list so you can use it in the next part. A good structure would be a list of lists where the inner list is one comment and its replies. (hint: you might need a TRIPLE nested for loop). This could take a minute or two once running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 2 CODING HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============= PART 3 =============\n",
    "\n",
    "The last part is to somehow use the data you have scraped from Reddit to run a HuggingFace model inference. \n",
    "\n",
    "### Todos for part 3:\n",
    "\n",
    "1. Look at each of these HuggingFace models and choose one to implement:\n",
    "    * [News Classification](https://huggingface.co/mrm8488/bert-mini-finetuned-age_news-classification) [HARD]\n",
    "    * [Sentiment Analysis](https://huggingface.co/docs/transformers/quicktour) [EASY]\n",
    "    * [Text Generation](https://huggingface.co/tasks/text-generation) [EASY]\n",
    "2. Once you have chosen a model, go ahead and implement it!\n",
    "3. If time permits, try another!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 3 CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the blanks for News Classification to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_tokenizer = AutoTokenizer.from_pretrained() # TODO: what gets passed here? hint: look at the docs!\n",
    "news_model = AutoModelForSequenceClassification.from_pretrained() # TODO: what gets passed here? hint: look at the docs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for News Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "NEWS_CLASSES = [\"world\", \"sports\", \"business\", \"sci/tech\"]\n",
    "\n",
    "# Take the output from our news classifier and map it to a class\n",
    "def map_news_output_to_class(self, inference_output: torch.Tensor) -> str:\n",
    "    softmax_values = []\n",
    "    for output in inference_output:\n",
    "        softmax_values.append(output.item())\n",
    "    max_value = max(softmax_values)\n",
    "    max_index = softmax_values.index(max_value)\n",
    "    return NEWS_CLASSES[max_index]\n",
    "\n",
    "# Run the news classifier model on the input\n",
    "def run_subject_analysis(self) -> str:\n",
    "    inputs = news_tokenizer(self.title, return_tensors=\"pt\")\n",
    "    labels = torch.tensor([1]).unsqueeze(0) # Batch size of 1\n",
    "    outputs = news_model(**inputs, labels=labels) # Unpack key-value pairs into keyword args in function call\n",
    "    news_subject = self.map_news_output_to_class(outputs.logits[0]) # Taking softmax tensor from inference\n",
    "    return news_subject\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70e5a9886b02a227d3c83e17263e3a28d12f842d6950be488144931078452c0e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('jims4thbrain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
